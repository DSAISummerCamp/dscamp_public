{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMCN5cSxPngG"
   },
   "source": [
    "<h1><center> Introduction to Natural Language Processing </center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dk3iOqmO-LkJ"
   },
   "source": [
    "Let's get started by setting up the workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22338,
     "status": "ok",
     "timestamp": 1591032288524,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "xORwVGdP-tWg",
    "outputId": "3634fd63-2a83-4e82-ceb9-707afa24c8f1"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-358f27e7180f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# mount the drive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# mount the drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4196,
     "status": "ok",
     "timestamp": 1591032564786,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "lz3kpE0S-x85",
    "outputId": "d2556670-58d3-4c0d-e5db-26f5d85a11b1"
   },
   "outputs": [],
   "source": [
    "## Change the working directory for the notebook\n",
    "%cd /content/gdrive/My Drive/dscamp/dscamp_public/NLP 1/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19674,
     "status": "ok",
     "timestamp": 1591032584962,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "5DvHeJ0Coc8e",
    "outputId": "75dfc6a7-a223-47c1-b7c9-83685668960e"
   },
   "outputs": [],
   "source": [
    "## Unzip the data \n",
    "!cat glove.6B.100d.tar.xz.parta* > glove.6B.100d.tar.xz\n",
    "!tar -xvf glove.6B.100d.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NrvYropQ-6DV"
   },
   "outputs": [],
   "source": [
    "import logging, warnings\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "my_dir = '/content/gdrive/My Drive/dscamp/dscamp_public/NLP 1'\n",
    "nltk.data.path.append(my_dir+\"/data/nltk_data/\")\n",
    "import sys\n",
    "sys.path.append(my_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kY44hPR-PngR"
   },
   "source": [
    "<font color='red'> <h2> What is NLP? </h2> </font> \n",
    "* One of the most challenging and revolutionary things artificial intelligence (AI) can do is speak, write, listen, and understand human language. Natural language processing (NLP) is a form of AI that extracts meaning from human language to make decisions based on the information.  \n",
    "<br>\n",
    "* It is an interdisciplinary field which draws on other areas of study such as computer science, Artificial Intelligence, linguistics.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/image001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2MiHbKIPngd"
   },
   "source": [
    "<font color='red'> <h2> Why NLP Understanding is hard? </h2> </font> \n",
    "* Natural language is extremely rich in form and structure, and very ambiguous.\n",
    "    - How to represent meaning\n",
    "    - Which structures map to which meaning structures.  \n",
    "<br>\n",
    "* One input can mean many different things. Ambiguity can be at different levels.\n",
    "    - Lexical (word level) ambiguity  -- different meanings of words\n",
    "    - Syntactic ambiguity  --  different ways to parse the sentence\n",
    "    - Interpreting partial information  --  how to interpret pronouns\n",
    "    - Contextual information  --  context of the sentence may affect the meaning of that sentence.  \n",
    "<br>\n",
    "* Many inputs can mean the same thing.  \n",
    "<br>\n",
    "* Interaction among components of the input is not clear. \n",
    "\n",
    "* How to represent language in a form that computers understand: \"hello world\" -> '1101000 1100101 1101100 1101100 1101111 100000 1110111 1101111 1110010 1101100 1100100'\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/image002.png)\n",
    "\n",
    "**What Applications of NLP can you think of?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6WyR8qk4Pngo"
   },
   "source": [
    "<font color='red'> <h2> NLP Applications </h2> </font> \n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/machine_translation.png)\n",
    "* Famous application: **Google Translate**\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/Picture1.png)\n",
    "* Movie Reviews: **Positive of Negative**\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/Picture2.png)\n",
    "* Classification of emails: **Important, Spam, etc**\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/Picture3.png)\n",
    "<font> <h3> Speech recognition (Speech-to-text) and Speech Understanding </h3> </font> \n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/speech_recog.jpeg' width=300/>\n",
    "</figure>\n",
    "\n",
    "* Famous application: **Amazon’s Alexa, Google’s Home, SIRI**\n",
    "\n",
    "<font> <h3> Text prediction </h3> </font> \n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/text_pred.png)\n",
    "\n",
    "* Speed up word processing and facilitate text dictation (seen in SMS and email)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SM_JB52Pngw"
   },
   "source": [
    "<font color='red'> <h2> NLP Overview </h2> </font> \n",
    "* NLP terminology\n",
    "* Text pre-processing\n",
    "* Word representation \n",
    "* Text representations\n",
    "* Sentiment analysis on the Amazon reviews using SVM\n",
    "* Sentiment analysis on the Amazon reviews using CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNhdunwmqHsA"
   },
   "source": [
    "<font color='red'> <h2> NLP terminology (sample dataset) </h2> </font>\n",
    "\n",
    "Let’s take an example to understand some terminology concepts in depth.\n",
    "\n",
    "1. “It was the best of times”\n",
    "2.   “It was the worst of times”\n",
    "3.   “It was the age of wisdom”\n",
    "4.   “It was the age of foolishness”\n",
    "\n",
    "Each sentence is a separate document and we make a list of all words from all the four documents excluding the punctuation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNbvy0Corbg8"
   },
   "source": [
    "<font color='red'> <h2> NLP terminology</h2> </font>\n",
    "\n",
    "**Corpus**\n",
    "\n",
    "In linguistics and NLP, corpus (plural corpora) refers to a collection of texts. Corpora are generally solely used for statistical linguistic analysis and hypothesis testing.\n",
    "\n",
    "**Tokenization** \n",
    "Tokenization is, generally, an early step in the NLP process, a step which splits longer strings of text into smaller pieces, or **tokens**. Tokens can be words, punctuations, numeric text etc. \n",
    "Here we consider words as tokens and remove whitespace to extract the tokens.\n",
    "\n",
    "1. “It” “was” “the” “best” “of” “times”\n",
    "2. “It” “was” “the” “worst” “of” “times”\n",
    "3. “It” “was” “the” “age” “of” “wisdom”\n",
    "4. “It” “was” “the” “age” “of” “foolishness”\n",
    "\n",
    "**Vocabulary** \n",
    "\n",
    "Set of all unique words (tokens) in a dataset.\n",
    "\n",
    "The set of unique words are :\n",
    "‘It’, ‘was’, ‘the’, ‘best’, ‘of’, ‘times’, ‘worst’, ‘age’, ‘wisdom’, ‘foolishness’\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScdPrtJx3jBP"
   },
   "source": [
    "<font color='red'> <h2> NLP terminology </h2> </font>\n",
    "\n",
    "**Stop Words:**\n",
    "\n",
    "Stop words are those words which are filtered out before further processing of text, since these words contribute little to the overall meaning, given that they are generally the most common words in a language. \n",
    "“a” , “the”, “have”, “has”, etc.    \n",
    "\n",
    "**N-gram:** \n",
    "\n",
    "For a fixed, small N (2-5 is common), an N-gram is a consecutive sequence of words in a text.\n",
    "For example, the bigrams in the first document : “It was the best of times” are as follows:\n",
    "“it was”     “was the”        “the best”       “best of”      “of times”\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "The allowable structures in the language: sentences, phrases, affixes (-ing, -ed, -ment, etc.).\n",
    "\n",
    "**Semantics:** \n",
    "\n",
    "The meaning(s) of texts in the language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ds9C0QtNPng3"
   },
   "source": [
    "<font color='red'> <h2> NLP Pipeline </h2> </font> \n",
    "<font color='black'> <h3> During this class we will go through the NLP pipeline where you can write your own text and follow the steps. We will perform sentiment analysis on amazon reviews dataset. You can write your own review and see if the model thinks it's positive or negative! </h3> </font> \n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/Capture.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xX2ZG5okPng_"
   },
   "source": [
    "<font color='red'> <h2> Text Pre-processing -  extracting the useful information from the textual data </h2> </font> \n",
    "\n",
    "* Converting all letters to lower or upper case\n",
    "\n",
    "* Converting numbers into words or removing numbers\n",
    "\n",
    "* Removing punctuations, accent marks and other diacritics\n",
    "\n",
    "* Removing white spaces\n",
    "\n",
    "* Removing stop words, sparse terms, and particular words\n",
    "\n",
    "* Stemming\n",
    "\n",
    "* Lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFIX2iROPnh5"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "def convert_to_lower(mytext):\n",
    "    return mytext.lower()\n",
    "def remove_numbers(mytext):\n",
    "    return ''.join([i for i in mytext if not i.isdigit()])\n",
    "def remove_punctuation(mytext):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return ' '.join([w.translate(table) for w in mytext.split()])\n",
    "def remove_white_spaces(mytext):\n",
    "    return ' '.join(mytext.strip().split())\n",
    "def remove_stop_words(mytext):\n",
    "    stop_words = list(stopwords.words('english')) \n",
    "    output =[]\n",
    "    return ' '.join([i for i in mytext.split() if not i in stop_words])\n",
    "def stemming(mytext):\n",
    "    stemmer= PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in mytext.split()])\n",
    "def lemmatization(mytext):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in mytext.split()])\n",
    "\n",
    "\n",
    "def pre_process(mytext):\n",
    "    mytext_lower = convert_to_lower(mytext)\n",
    "    my_text_char = remove_numbers(mytext_lower)\n",
    "    my_text_no_punct = remove_punctuation(my_text_char)\n",
    "    my_text_white_spaces = remove_white_spaces(my_text_no_punct)\n",
    "    preprocessed_text = remove_stop_words(my_text_white_spaces)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bat-fNefPnie"
   },
   "source": [
    "<font color='red'> <h2> Write your text below and test our pre-processing functions! </h2> </font> \n",
    "\n",
    "###  Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1139,
     "status": "ok",
     "timestamp": 1591032606230,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "PFbRc_G2Scfh",
    "outputId": "402baffd-f4e9-4df6-8992-563e5ad272d1"
   },
   "outputs": [],
   "source": [
    "mytext = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
    "print(convert_to_lower(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iF5ArJRAUbQZ"
   },
   "source": [
    "###  Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1591032606607,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "wIrPdA1qUTsY",
    "outputId": "243a819f-482c-4126-8229-7014ce802310"
   },
   "outputs": [],
   "source": [
    "mytext = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\n",
    "print(remove_numbers(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5xEC2LbUvHs"
   },
   "source": [
    "###  Remove punctuation\n",
    "The following code removes this set of symbols \"[ ! ” # $ % & ’ () * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~ ]:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1591032608260,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "sWIPlyGJU0ZL",
    "outputId": "50dd4fa4-4741-4f24-cea5-5b4010967fc0"
   },
   "outputs": [],
   "source": [
    "mytext = \"This &is [an] example? {of} string. with.? punctuation!!!!\"\n",
    "print(remove_punctuation(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WiatHvQxVNww"
   },
   "source": [
    "###  Remove whitespaces\n",
    "The following removes leading and ending spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 670,
     "status": "ok",
     "timestamp": 1591032608886,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "aACUtJbRVpCf",
    "outputId": "84e1be77-19be-42b8-cefd-790e143a958b"
   },
   "outputs": [],
   "source": [
    "mytext = \" \\t a string example\\t \"\n",
    "print(remove_white_spaces(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6h1vJbHVaw2D"
   },
   "source": [
    "#### Remove Stopwords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3875,
     "status": "ok",
     "timestamp": 1591032613428,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "X37TxpYXa1UZ",
    "outputId": "82cfd4b8-82a2-4303-cd51-ea4ab1c1315c"
   },
   "outputs": [],
   "source": [
    "mytext = 'NLTK is a leading platform for building Python programs to work with human language data.'\n",
    "print(remove_stop_words(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c1giOB5jbRJO"
   },
   "source": [
    "###  Stemming\n",
    "\n",
    "Stemming is a process of reducing words to their word stem, base or root form. \n",
    "Some examples of stemming for root word \"like\" include:\n",
    "\n",
    "->\"likes\"\n",
    "\n",
    "->\"liked\"\n",
    "\n",
    "->\"likely\"\n",
    "\n",
    "->\"liking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2582,
     "status": "ok",
     "timestamp": 1591032613429,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "GjXcSB5mbW4Q",
    "outputId": "3214a0b9-b952-4b28-d7c1-5229d4c63142"
   },
   "outputs": [],
   "source": [
    "mytext = 'There are several types of stemming algorithms.'\n",
    "print(stemming(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5afSzHdzfTe_"
   },
   "source": [
    "###  Lemmatization\n",
    "\n",
    "The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words.\n",
    "\n",
    "Lemmatization tools are presented libraries described above: NLTK (WordNet Lemmatizer), spaCy, TextBlob, Pattern, gensim, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), Illinois Lemmatizer, and DKPro Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10301,
     "status": "ok",
     "timestamp": 1591032622347,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "g8CLWNISfnms",
    "outputId": "4bd309c0-dedb-4dc1-9aac-6f867a25bb13"
   },
   "outputs": [],
   "source": [
    "mytext = 'There are several types of stemming algorithms.'\n",
    "print(lemmatization(mytext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6UuQZfdPnjN"
   },
   "source": [
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/Capture2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TekEq7-sPnjT"
   },
   "source": [
    "<font color='red'> <h2>How do we have usable meaning in a computer?</h2> </font> \n",
    "\n",
    "Definition: meaning (Webster dictionary)\n",
    "<br/>\n",
    "• the idea that is represented by a word, phrase, etc.\n",
    "<br/>\n",
    "• the idea that a person wants to express by using\n",
    "words, signs, etc.\n",
    "<br/>\n",
    "• the idea that is expressed in a work of writing, art, etc.\n",
    "<br/>\n",
    "How do we represent words? Remember, the computer only understands numbers ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "By95j_CnPnjY"
   },
   "source": [
    "### Solution 1: Representing words as discrete symbol\n",
    "Example: Very entertaining and funny!\n",
    "<br/>\n",
    "* We define the vocabulary: \n",
    "\\begin{align}\n",
    "\\mathcal V =\\{'entertaining','funny'\\}\\\\\n",
    "\\end{align}\n",
    "* Embeddings: \n",
    "\\begin{equation}\n",
    "  emb =\n",
    "    \\begin{cases}\n",
    "      X^{entertaining} = [1,0]\\\\\n",
    "      X^{funny} = [0,1]\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "* Vector dimension = number of words in vocabulary (e.g., 500,000)\n",
    "* This Method is called **<font color='red'>One Hot Encoding**</font> : <br/>\n",
    "For a corpus 𝐶 with finite vocabulary, 𝑉 and |𝑉|=𝑛.\n",
    "<br/> Let emb:𝑉→ℕ×ℕ⋯×ℕ≐𝑉 ̃  be a map defined by taking every element in 𝑉 to an 𝑛 component object 𝑋 ⃗∈𝑉 ̃\n",
    " such that: <br/>\n",
    "    \\begin{equation}\n",
    "  X_{i}^{w} =\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if    } idx(w)=i \\\\\n",
    "      0,& \\text{otherwise}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tAd-X7VUPnj7"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def discrete_word_representation(mytext):\n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(mytext.split())\n",
    "    # binary encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    return onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6886,
     "status": "ok",
     "timestamp": 1591032622349,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "XEYV0xbhkCoP",
    "outputId": "f3127fb6-ac55-4041-eb55-62849bc77d29"
   },
   "outputs": [],
   "source": [
    "# We pre-process our product review! \n",
    "positive_review= \"Very entertaining and funny!\"\n",
    "processed_positive_review = pre_process(positive_review)\n",
    "print(processed_positive_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6234,
     "status": "ok",
     "timestamp": 1591032622350,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "uw2LbbDdPnkX",
    "outputId": "f2409fd7-3cb5-46a4-9a48-41cdc68fd4ae"
   },
   "outputs": [],
   "source": [
    "one_hot_vec = discrete_word_representation(processed_positive_review)\n",
    "one_hot_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7269,
     "status": "ok",
     "timestamp": 1591032624180,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "GUHHlJPyPnkz",
    "outputId": "4d959bfd-e65f-4288-e3c1-91ea4ce7f411"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from utilities import *\n",
    "figure()\n",
    "new_blank_plot(title='words as discrete symbols')\n",
    "draw_vector2d(one_hot_vec[0], color='blue')\n",
    "draw_vector2d(one_hot_vec[1], color='red')\n",
    "draw_label2d(one_hot_vec[0], processed_positive_review.split()[0], color='blue', coords=True)\n",
    "draw_label2d(one_hot_vec[1], processed_positive_review.split()[1], color='red', coords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wRDQskIVPnlP"
   },
   "source": [
    "** These 2 vectors are <font color='red'>orthogonal</font>** <br/>\n",
    "** There is no natural notion of <font color='red'> similarity</font> for one-hot vectors!**\n",
    "<h2>  Learn to encode similarity in the vectors themselves !</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJQ_h0ySPnlV"
   },
   "source": [
    "### Solution 2: Representing words by their context\n",
    "* **Distributional semantics: A word’s meaning is given by the words that frequently appear close-by**\n",
    "* When a word w appears in a text, its context is the set of words that appear nearby (within a fixed-size window).\n",
    "* Use the many contexts of $w$ to build up a representation of $w$\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/Context_representation.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWtrzDWZPnlb"
   },
   "source": [
    "<font color='red'> <h2> Word Vectors/Embeddings</h2> </font> \n",
    "* We will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.\n",
    "<br/>\n",
    "* **Lower dimensional** -> We consider a word embedding to a D dimensional vector space where D is less than the size of the vocabulary. (in practice it is a linear projection from the embedding space created by one hot encoding and D is no more than 500).\n",
    "<br/>\n",
    "* **“Semantic and Syntax”** -> Word vectors will be allowed to be non-orthogonal. In non-mathematical terms we will preserve the semantic similarity of words by using the direction of the word vectors.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/linear-relationships.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iid5GDEXH8DG"
   },
   "source": [
    "<font color='red'> <h2> Word2Vec Efficient high dimensional word embeddings </h2> </font>\n",
    "\n",
    "Two related models: \n",
    "* Skip-Gram model: predict context from the context words (position independent) within a fixed window given center word\n",
    "* Continuous Bag of Words model (CBOW): predict center word from (bag of) context words\n",
    "\n",
    "Example: \"... rescue by the Treasury and Fed...\" with a window size of 2\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/word2vec_models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "105RyRYAPnli"
   },
   "source": [
    "<font color='red'> <h2> Word2Vec </h2> </font>\n",
    "We expand on Skip-Gram model. The idea:\n",
    "\n",
    "• We have a large corpus of text \n",
    "\n",
    "• Every word in a fixed vocabulary is represented by a vector\n",
    "\n",
    "• Go through each position t in the text, which has a center word $w_t$ and context of words within a fixed window size\n",
    "\n",
    "• Use the similarity of the word vectors for the center word $w_t$ and context word to calculate the probability of context given center (or vice versa)\n",
    "\n",
    "• Keep adjusting the word vectors to maximize this probability\n",
    "\n",
    "Example\twindows\tand\tprocess\tfor\tcomputing $P(w_{t+j}|w_t)$\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/example_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5uvhR_r0lRKK"
   },
   "source": [
    "<font color='red'> <h2> How do we get these vectors? </h2> </font> \n",
    "\n",
    "* We append all the documents, then iterate through every word t in the corpus, predict the context words within the fixed window size m, given the center word:\n",
    "The likelihood $L(\\theta)=\\prod_{t=1}^{T} \\prod_{-m\\le j \\le m}^{T}P(w_{t+j}|w_t;\\theta)$\n",
    "\n",
    "* The parameters $\\theta$ are all the vector embeddings. $\\theta$ is one long vectyor of size $d*V$ where $d$ is the size of the embedding vectors and $V$ is the vocabulary \n",
    "\n",
    "* We obtain the embeddings by minimizing the objective function defined as the average negative log likelihood: $J(\\theta)=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m\\le j \\le m}^{T}logP(w_{t+j}|w_t;\\theta)$\n",
    "\n",
    "* By minimizing the objective function, we maximize the predictive accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "doq47X9gJXk2"
   },
   "source": [
    "<font color='red'> <h2> How do we get these vectors? -- Gradient Descent </h2> </font>\n",
    "\n",
    "We use gradient descent to minimize the cost function $J(\\theta)$.\n",
    "* Start with random value of $\\theta$\n",
    "* Calculate gradient of $J(\\theta)$\n",
    "* Take a step in the direction of negative gradient\n",
    "* Repeat\n",
    "$$ \\theta_{new} = \\theta_{old} - \\alpha \\nabla(J\\theta)$$\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/grad_descent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4429,
     "status": "ok",
     "timestamp": 1591032719412,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "QlgL_w72Pnln",
    "outputId": "50a66966-02d9-44be-9f58-8edb75dc3d91"
   },
   "outputs": [],
   "source": [
    "# Read pre-trained 300-d word2vec embeddings of the reviews corpus \n",
    "import pandas as pd\n",
    "\n",
    "embeddings_vocab = pd.read_csv(my_dir+'/data/embeddings_vocab.csv', index_col=0)\n",
    "print('embeddings_vocab size: {}'.format(embeddings_vocab.shape))\n",
    "embeddings_vocab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qfwb3v8gPnmG"
   },
   "source": [
    "<font color='red'> <h2> Projection on 2-D for visualization </h2> </font> \n",
    "<font color='red'> <h2> We use t-SNE to project the word embeddings from 300 d to 2D </h2> </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9LmPvBQPnna"
   },
   "outputs": [],
   "source": [
    "colors=['blue','red','green','yellow','brown','orange']\n",
    "def visualize_words_w2v(my_list_of_words):\n",
    "    figure()\n",
    "    for i in range(len(my_list_of_words)):\n",
    "        if my_list_of_words[i] in tsne_embeddings['token'].values:\n",
    "            new_blank_plot(title='words embeddings Word2Vec',xlim=(-3, 3), ylim=(-3, 3))\n",
    "            draw_vector2d(list(tsne_embeddings[tsne_embeddings['token']==my_list_of_words[i]].values[0][1:]), color=colors[i])\n",
    "            draw_label2d(list(tsne_embeddings[tsne_embeddings['token']==my_list_of_words[i]].values[0][1:]), my_list_of_words[i], color=colors[i], coords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2168,
     "status": "ok",
     "timestamp": 1591032819859,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "hFTwKiNiPnoT",
    "outputId": "5e219572-e797-4f23-8a4e-0868b09b6319"
   },
   "outputs": [],
   "source": [
    "# Load the 2D embeddings \n",
    "tsne_embeddings = pd.read_csv(my_dir+'/data/tsne_embeddings.csv', index_col=0)\n",
    "# visualize a list of words \n",
    "visualize_words_w2v(processed_positive_review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2106,
     "status": "ok",
     "timestamp": 1591032820474,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "p4IktzUQ811B",
    "outputId": "1fce6d2f-d0b6-4cc5-fa43-558cd09fccc2"
   },
   "outputs": [],
   "source": [
    "# visualize a list of words \n",
    "my_neg_review = 'the package is damaged and table and chairs are broken'\n",
    "visualize_words_w2v(pre_process(my_neg_review).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzCuIJusLIf4"
   },
   "source": [
    "<font color='red'> <h2> GloVe: Global Vectors </h2> </font> \n",
    "\n",
    "Another word embedding method. \n",
    "\n",
    "Uses global word-word co-occurrence matrix and trains a log-bilinear model with a weighted least-squares objective.\n",
    "\n",
    "<font color='red'> **We will use pretrained GloVe embeddings of dimension 100 to see some exciting results** </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQxLgXmDMeMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lo8VZyWVMnIW"
   },
   "outputs": [],
   "source": [
    "glove_file = datapath(my_dir+'/data/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41481,
     "status": "ok",
     "timestamp": 1591032873639,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "Q6CmqW7RNrLH",
    "outputId": "e4897a8a-723b-4aeb-c6be-6bb388344eb6"
   },
   "outputs": [],
   "source": [
    "model.most_similar('summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40541,
     "status": "ok",
     "timestamp": 1591032873640,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "CscBDS4mNvKc",
    "outputId": "a3708ec1-6add-44c7-cec5-3de57fe77a63"
   },
   "outputs": [],
   "source": [
    "model.most_similar('movie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THcIWGFuN0aw"
   },
   "source": [
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/word_analogies.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFETZhEcPCI-"
   },
   "outputs": [],
   "source": [
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39115,
     "status": "ok",
     "timestamp": 1591032873642,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "tmguJKhPPJjO",
    "outputId": "b6fa217e-6e28-4bc4-fc3e-b6aa9737be49"
   },
   "outputs": [],
   "source": [
    "analogy('man', 'king', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38591,
     "status": "ok",
     "timestamp": 1591032873642,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "s26iU7wEPULz",
    "outputId": "08d5fd92-83ab-492e-c2cd-c97e9e84ea76"
   },
   "outputs": [],
   "source": [
    "analogy('england', 'english', 'italy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38021,
     "status": "ok",
     "timestamp": 1591032873643,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "8g_Vz8IyPWsx",
    "outputId": "d8f823b2-a836-4b8b-e93d-f14b81ec9cf9"
   },
   "outputs": [],
   "source": [
    "analogy('america', 'baseball', 'france')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVNk1ZRdPnos"
   },
   "source": [
    "<font color='red'> <h2> NLP Pipeline </h2> </font> \n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/nlp_pipeline_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e06d1sfvPnov"
   },
   "source": [
    "<font color='red'> <h2> Transformation from words to sentences/text</h2> </font> \n",
    "* Take average of the word2vecs of its words (used in the first example SVM)\n",
    "* Concatenate the word embeddings to form the sentence (used in the second example CNN)\n",
    "* Another approach: Paragraph vector (2014, Quoc Le, Mikolov)\n",
    "  * Extend word2vec to text level\n",
    "  * Also two models: add paragraph vector as the input\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/sentence_embed.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 92894,
     "status": "ok",
     "timestamp": 1591032929527,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "iTr6doRgPnoz",
    "outputId": "cc34d1b6-dc7f-49b1-a008-65278e731756"
   },
   "outputs": [],
   "source": [
    "# Read the amazon reviews dataset\n",
    "data = pd.read_csv(my_dir+'/data/amazon_data_10k.csv', index_col=0)\n",
    "# pre_process the reviews dataset\n",
    "data['reviews'] = data['reviews'].apply(lambda x: pre_process(x))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 104395,
     "status": "ok",
     "timestamp": 1591032941490,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "nU4yui9KPnpO",
    "outputId": "5641fd37-3bf0-4589-e84d-ad2559d8bfa4"
   },
   "outputs": [],
   "source": [
    "### Represent reviews as average of words embeddings\n",
    "\n",
    "all_review = []\n",
    "all_unigram = []\n",
    "\n",
    "for a in data['reviews']:\n",
    "    unigram = []\n",
    "    review = []\n",
    "    unigram = a.split()\n",
    "    review = [a] * len(unigram)\n",
    "    all_review.extend(review)\n",
    "    all_unigram.extend(unigram)\n",
    "\n",
    "\n",
    "df_unigram = pd.DataFrame({'reviews': all_review, 'token': all_unigram})\n",
    "df_unigram = pd.merge(data, df_unigram, how='right', on=['reviews'])\n",
    "df_unigram = df_unigram.drop_duplicates(subset = ['reviews','token'])\n",
    "df_unigram = pd.merge(embeddings_vocab, df_unigram, how='right', on=['token']).dropna()\n",
    "df_unigram = df_unigram.groupby(['reviews'],as_index = False).mean()\n",
    "df_unigram.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEmoiLJZPnpj"
   },
   "source": [
    "<font color='red'> <h2> Simple Classifier – From word representation to modeling </h2> </font> \n",
    "**Linear SVM and Logistic Based Classification**\n",
    "* Pre-processing converts words to features and creates new features based on word count\n",
    "* Text vectorization  outputs  the features to numerical vectors. Ex count vectors, TF-IDF vectors\n",
    "* For classification problems, vector spaced based ML methods can be applied to find decision boundary between two classes .  Notable example SVM.\n",
    "* Linear SVM defines the criterion that maximally separates the two classes, allowing users to adjust cost and penalty parameters on misclassification to suit business problems.\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/SVM_hyperplane.png' width=300/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 108422,
     "status": "ok",
     "timestamp": 1591032946403,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "lLT2nrIbPnpo",
    "outputId": "5146d8b7-a696-4b63-c879-90dfe0a8a688"
   },
   "outputs": [],
   "source": [
    "### Split the dataset to train and test (80% for training and 20% for testing)\n",
    "\n",
    "## Fit the linear SVC on the training data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(df_unigram[df_unigram.columns[1:301]],df_unigram['sentiment'],test_size=0.2)\n",
    "clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf.fit(Train_X, Train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 107862,
     "status": "ok",
     "timestamp": 1591032946405,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "oAgyf6FEPnp4",
    "outputId": "8c2c48fd-e7c3-4361-ced1-b57383499205"
   },
   "outputs": [],
   "source": [
    "# Test the Linar SVC model on the test data\n",
    "# WE use the accuracy defined as ratio of correctly predicted data by the overall data\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Test_predictions = clf.predict(Test_X)\n",
    "print(\"The model predicted {0:.2f}% correctly from the test dataset \".format(accuracy_score(Test_Y,Test_predictions)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Xil5Zz2PnqL"
   },
   "source": [
    "<font color='red'> <h2> Cassify the sentiment of your own review using the linear SVC model! </h2> </font> \n",
    "The linear SVM is able to correctly classify **83.29%** of the amazon test dataset reviews.\n",
    "<br/>\n",
    "You can use the function below to find out what the model thinks of your review!!\n",
    "\n",
    "1) Write your review\n",
    "\n",
    "2) Preprocess your review\n",
    "\n",
    "3) Call the function get_svc_class(my_processed_review) to ge the result of the SVM model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnZstxBwPnqf"
   },
   "outputs": [],
   "source": [
    "# Classify the sentiment of my comment\n",
    "import numpy as np\n",
    "def get_svc_class(my_processed_review):\n",
    "    avg_embed = []\n",
    "    for i in my_processed_review.split():\n",
    "        if i in embeddings_vocab['token'].values:\n",
    "            avg_embed.append(embeddings_vocab[embeddings_vocab['token']=='happy'].values[0][1:])\n",
    "    my_prediction = clf.predict(np.mean(avg_embed,axis=0).reshape(1, -1))\n",
    "    if my_prediction == 1:\n",
    "        print('The model predicted that the review is positive')\n",
    "    else:\n",
    "        print('The model predicted that the review is negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1777,
     "status": "ok",
     "timestamp": 1591032948203,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "jHk26qVvPnq1",
    "outputId": "1e2eb1b2-cd9b-449f-b795-e6e5719d89e6"
   },
   "outputs": [],
   "source": [
    "get_svc_class(processed_positive_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6BhGRAUbPnrb"
   },
   "source": [
    "<font color='red'> <h2> Neural network classifier - convolution neural networks </h2> </font> \n",
    "* Traditionally, convolution neural networks (CNNs) are used to analyze images and are made up of one or more convolutional layers.\n",
    "\n",
    "* Main idea here is to use multiple filters of different sizes that can look at bi-grams, tri-grams, n-grams.\n",
    "\n",
    "* Fast to train, works well, but fails to capture longer dependencies\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/PDmitriy/dscamp_public/master/NLP%201/images/CNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CrIPstCtPnry"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2o5VZdiPnsW"
   },
   "outputs": [],
   "source": [
    "# make the max word length to be constant\n",
    "MAX_WORDS = embeddings_vocab.shape[0]\n",
    "max_sequence_length = 200\n",
    "embedding_dim = 300\n",
    "filter_sizes = [1,2,3]\n",
    "num_filters = 20\n",
    "drop = 0.3\n",
    "batch_size = 64\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4886,
     "status": "ok",
     "timestamp": 1591032951337,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "5rexKstKPnst",
    "outputId": "30013c70-eade-447e-806a-0f55bf7f8826"
   },
   "outputs": [],
   "source": [
    "tokenizer  = Tokenizer(num_words = MAX_WORDS)\n",
    "tokenizer.fit_on_texts(data['reviews'])\n",
    "sequences =  tokenizer.texts_to_sequences(data['reviews'])\n",
    "word_index = tokenizer.word_index\n",
    "print(\"unique words : {}\".format(len(word_index)))\n",
    "data_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "## Split data to train, validation and test\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(data_padded, data['sentiment'],test_size=0.2)\n",
    "Train_X, Val_X, Train_Y, Val_Y = train_test_split(Train_X, Train_Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0Pnz9K7PntL"
   },
   "outputs": [],
   "source": [
    "## Create embeddings Matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in embeddings_vocab['token'].values:\n",
    "        embedding_matrix[i] = embeddings_vocab[embeddings_vocab['token']==word].values[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlPSZgMR29V3"
   },
   "outputs": [],
   "source": [
    "model = CNN_Model(word_index=word_index, embedding_dim=embedding_dim,embedding_matrix=embedding_matrix, num_filters=num_filters, filter_sizes=filter_sizes, max_sequence_length=max_sequence_length, drop=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 109926,
     "status": "ok",
     "timestamp": 1591033056396,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "IGldUOWh3Ilt",
    "outputId": "bc1937a2-59e4-4527-cd63-d9cb925afe15"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 159074,
     "status": "ok",
     "timestamp": 1591033105553,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "jIwAxQlWPnuX",
    "outputId": "5d6025b7-f615-42b0-c5a7-e6d50bb8761e"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "history = model.fit(Train_X, Train_Y, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(Val_X, Val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3856,
     "status": "ok",
     "timestamp": 1591033188394,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "29it9sh2Pnum",
    "outputId": "504ffe8d-0344-4ad9-9868-15d6daa89d9d"
   },
   "outputs": [],
   "source": [
    "model = load_model(my_dir+'/weights_cnn_sentece.hdf5')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2121,
     "status": "ok",
     "timestamp": 1591033191556,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "2NmOo75EPnu_",
    "outputId": "143c7e7f-c5f8-4609-cc26-2fd784a8f43f"
   },
   "outputs": [],
   "source": [
    "print(\"The model predicted {0:.2f}% correctly from the test dataset \".format(accuracy_score(Test_Y, (model.predict(Test_X) > 0.5).astype(int))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mkj0zjTB9WUy"
   },
   "source": [
    "**You can write multiple sentences and see if the model thinks the review is positive or negative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GQQOkgIdPnvO"
   },
   "outputs": [],
   "source": [
    "def CNN_class(my_text):\n",
    "    test_sequences =  tokenizer.texts_to_sequences([my_text])\n",
    "    data_padded_my_text = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "    if model.predict(data_padded_my_text)>=0.5:\n",
    "        print('The model predicted that the review is positive')\n",
    "    else:\n",
    "        print('The model predicted that the review is negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1068,
     "status": "ok",
     "timestamp": 1591033195516,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "cM2MndWK8u8L",
    "outputId": "bac810ce-7224-4ea9-e612-9f358cee498a"
   },
   "outputs": [],
   "source": [
    "my_review = 'This computer is great! I love it!'\n",
    "CNN_class(my_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1398,
     "status": "ok",
     "timestamp": 1591033196883,
     "user": {
      "displayName": "wafa louhichi",
      "photoUrl": "",
      "userId": "16722855635351543947"
     },
     "user_tz": 240
    },
    "id": "aoAlfHuf6VPV",
    "outputId": "9a7b8448-1ceb-462d-eb99-ce119aa5e921"
   },
   "outputs": [],
   "source": [
    "my_review_2 = 'Very disappointed with the product, the quality is poor'\n",
    "CNN_class(my_review_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beZv4BxM9yAk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP summer_camp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
