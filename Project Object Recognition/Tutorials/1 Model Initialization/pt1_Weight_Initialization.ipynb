{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanshenwang/dscamp_public/blob/master/Project%20Object%20Recognition/Tutorials/1%20Model%20Initialization/pt1_Weight_Initialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXvcf0k1PM_z"
      },
      "source": [
        "# Neural Networks Weight Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSPwsuDfPM_2"
      },
      "source": [
        "Training your neural network requires specifying an initial value of the weights. \n",
        "\n",
        "A well chosen initialization can:\n",
        "<ul>\n",
        "<li>Speed up the convergence of gradient descent\n",
        "<li>Increase the odds of gradient descent converging to a lower training (and generalization) error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NHWGg5zPM_2"
      },
      "source": [
        "We consider three possible weight initialization methods in this module:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<ul>\n",
        "<li> <i>Zero Initialization</i> -- Assigns an initial weight of zero to all the weights.\n",
        "    <li> <i> Random Initialization </i> -- This initializes the weights to large random values.\n",
        "    <li> <i> He Initialization </i> --  This initializes the weights to random values scaled according to a paper by He et al., 2015. \n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N7INExiPM_3"
      },
      "source": [
        "These three initialization methods were applied on a three layer neural network which tried to separate the blue dots from the red dots as shown in the image below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oANLp0ZbPM_3"
      },
      "source": [
        "![title](https://raw.githubusercontent.com/mahadevprakash90/dscamp_public/Aditya/Project%20Object%20Recognition/Tutorials/img/input_image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNYO09wWPM_4"
      },
      "source": [
        "Now we can see how each initialization methods perform in their classification tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkkiB-4dPM_4"
      },
      "source": [
        "There are two types of parameters to initialize in a neural network:\n",
        "<ul>\n",
        "<li>the weight matrices (${W^{[1]},W^{[2]},W^{[3]},\\dots,W^{[L-1]},W^{[L]}}$)\n",
        "<li>the bias vectors ($b^{[1]},b^{[1]},b^{[1]}.\\dots,b^{[L-1]},b^{[L]}$)\n",
        "    </ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "79JHZE3MjBHA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElcTxKGHPM_4"
      },
      "source": [
        "## 1. Zero Weight Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ni8L2zm1kzZd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWUH795dPM_5"
      },
      "source": [
        "We see in the plot below that the cost function is not getting updated during the training phase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoClZeUbPM_5"
      },
      "source": [
        "![zero_weights_performance](https://raw.githubusercontent.com/mahadevprakash90/dscamp_public/Aditya/Project%20Object%20Recognition/Tutorials/img/zero_weights_performance.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf2xZDyPPM_6"
      },
      "source": [
        "We can also see the decision boundary obtained after training the model (We would expect a boundary that separates the red dots from blue dots):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYzb8GtPM_6"
      },
      "source": [
        "![zero_weight_result](https://raw.githubusercontent.com/mahadevprakash90/dscamp_public/Aditya/Project%20Object%20Recognition/Tutorials/img/zero_weights_result.png)\n",
        "\n",
        "The decision boundary is -- there is no boundary.\n",
        "\n",
        "Why? because the model has the exactly same prediction for every example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYntYbVyPM_6"
      },
      "source": [
        "\n",
        "\n",
        "Initializing weights to a same value make the network fail to break symmetry. Which means that every neuron in each layer will learn the same thing. In the other words, the network is no more powerful than a linear classifier such as logistic regression.\n",
        "\n",
        "Below is a simple neural network:\n",
        "\n",
        "![title](https://qph.cf2.quoracdn.net/main-qimg-c63faaef309939f9bfa4887b4ed537df)\n",
        "\n",
        "In the hidden layer, all neural (blue circles) will learn the exactly same value as they all have the exactly same starting point!\n",
        "\n",
        "Even worse, for some activation functions, zero initizations will not help the algorithm to make any moves. Then the whole network will not learn anything but zeros.\n",
        "\n",
        "\n",
        "<b>What you should remember</b>:\n",
        "<ul>\n",
        "<li>The weights ${W^{[l]}}$ should NOT be initialized at the same value including zero. Instead, a random initialization is good to break symmetry.\n",
        "<li>It is however okay to initialize the biases ${b^{[l]}}$ to zeros. Symmetry is still broken so long as ${W^{[l]}}$ is initialized randomly.\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "815A4zrEc3N0"
      },
      "source": [
        "**Pros and Cons of zero initialization:**\n",
        "\n",
        "Pros:\n",
        "1.   Convenient? At least it is fast to train.\n",
        "\n",
        "Cons:\n",
        "1.   Only work for linear problems. (But we need a neural network!)\n",
        "2.   Sometimes it totally learns nothing.\n",
        "\n",
        "It is basically a bad idea to do. Do not initialize weights to zeros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGgorcVHPM_7"
      },
      "source": [
        "## 2. Random Weight Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF5oL7egPM_7"
      },
      "source": [
        "Now we know weights have to been set up differently. In this way, each neuron can then proceed to learn a different function of its inputs. \n",
        "\n",
        "Now, what happens if the weights are intialized randomly, but to very large values?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3pj6RezPM_7"
      },
      "source": [
        "Each weight matrix is initialized with the values ```np.random.randn( layers_dims[l],layers_dims[l-1] ) * 10```. This generates normally distributed random variables with mean 0 and standard deviation and multiplies the resulting random number by 10. The bias vectors are initialized with zero values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrNAxXV7PM_7"
      },
      "source": [
        "![random_weights_performance](https://raw.githubusercontent.com/mahadevprakash90/dscamp_public/Aditya/Project%20Object%20Recognition/Tutorials/img/random_weights_performance.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RARV_iHhEsQ"
      },
      "source": [
        "Good news: the cost function curve shows the network is learning! Cost is getting lower as we run more iterations.\n",
        "\n",
        "\n",
        "Now we have a decision boundary to tell what the model predicts for each data point.\n",
        "\n",
        "Bad news: it takes a long time to run especially those random numbers are too large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUuJimiOPM_7"
      },
      "source": [
        "![random_weight_result](https://raw.githubusercontent.com/mahadevprakash90/dscamp_public/Aditya/Project%20Object%20Recognition/Tutorials/img/random_weights_results.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc4bpqQjPM_8"
      },
      "source": [
        "<b>What to know</b>:\n",
        "<ul>\n",
        "<li>Random initialization make the neural network do its job: each neuron is able to learn differently.\n",
        "\n",
        "<li>But we need to pay attention on data scales: if we pick large values to start, the gradient (will be introduced in the next session) will be too large to handle. This is called gradient explosion, which cost huge to train the model but only to get unstable results.\n",
        "\n",
        "<li>What about starting from small values? Values may be too small to make a signficant move. This is called gradient varnish.\n",
        "\n",
        "\n",
        "A very good explanation on gradient explosion and varnishing:\n",
        "\n",
        "https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58GusHFHeaTY"
      },
      "source": [
        "**Pros and Cons of random initialization:**\n",
        "\n",
        "Pros:\n",
        "\n",
        "1.   Allows the neural network to learn more complex functions\n",
        "2.   Easy to implement\n",
        "\n",
        "Cons:\n",
        "\n",
        "1.   Initializing to large weights may leads to huge calculation costs, therefore the result will be unstable.\n",
        "2.   Smaller initial weights will slow down the optimization process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZTjbpEdPM_8"
      },
      "source": [
        "## 3. He Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GybiTSWGPM_8"
      },
      "source": [
        "Finally, we try \"He Initialization\"; this is named for the first author of He et al., 2015. (If you have heard of \"Xavier initialization\", this is similar except Xavier initialization uses a scaling factor for the weights  of ```sqrt(1./layers_dims[l-1])``` where He initialization would use ```sqrt(2./layers_dims[l-1]).)```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UU3ZTgNPM_8"
      },
      "source": [
        "The only difference is that instead of multiplying ```np.random.randn(..,..) by 10```, we multiply it by $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$, which is what He initialization recommends for layers with a ReLU activation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGPH-rpTPM_8"
      },
      "source": [
        "![he_weights_performance](https://raw.githubusercontent.com/mahadevprakash90/dscamp_public/Aditya/Project%20Object%20Recognition/Tutorials/img/he_weights_performance.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMo4QflSi5qw"
      },
      "source": [
        "After training, it works like a charm!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQNljBGGPM_9"
      },
      "source": [
        "![he_weights_results](https://raw.githubusercontent.com/mahadevprakash90/dscamp_public/Aditya/Project%20Object%20Recognition/Tutorials/img/he_weights_result.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTSkFYGCPM_9"
      },
      "source": [
        "<b>Observations:</b>\n",
        "<ul>\n",
        "<li>The model with He initialization separates the blue and the red dots very well in a small number of iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOCRIPTbjUZN"
      },
      "source": [
        "**Pros and Cons of He initialization:**\n",
        "\n",
        "Pros:\n",
        "\n",
        "1.   Allows fast optimization\n",
        "2.   Can learn complex and highly non-linear patterns\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAOaTgfCPM_9"
      },
      "source": [
        "## 4. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFiefKw2PM_9"
      },
      "source": [
        "We haves seen three different types of initializations. For the same number of iterations and same hyperparameters the comparison is:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ8uLCR0PM_9"
      },
      "source": [
        "|Model|Train Accuracy| Comment|\n",
        "|---|---|---|\n",
        "|3-layer NN with zeros initialization|50%|fails to break symmetry|\n",
        "|3-layer NN with large random initialization|83%|too large weights|\n",
        "|3-layer NN with He initialization|99%|recommended method|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcd3QyKuPM_9"
      },
      "source": [
        "What you should remember from this notebook:\n",
        "<ul>\n",
        "<li>Different initializations lead to different results\n",
        "<li>Random initialization is used to break symmetry and make sure different hidden units can learn different things\n",
        "<li>Don't intialize to values that are too large\n",
        "<li>He initialization works well for networks with ReLU activations."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}